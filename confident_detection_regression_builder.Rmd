---
Title: "QuantMeta: Confident Detection Regression Builder"
Author: "Dr. Kathryn Langenfeld"
Output: html_notebook
---

Purpose: This R notebook is designed to develop a regression to determine E_detect (minimum E_rel) for confident detection with respect to a target's length. The regression from Langenfeld et al. (2022) may be adopted, but the minimum E_rel is based on the standards proposed by FastViromeExplorer (Lithi et al. 2018) (10% read coverage and 0.3 observed/expected read distribution). If these parameters are deemed inappropriate for specific applications, it is recommended that new E_detect regressions be created for specific research needs.

Load required R packages
```{r}
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("MASS")
#install.packages("scales")
#install.packages("gridExtra")
#install.packages("cowplot")
#install.packages("ggpmisc")
#install.packages("pROC")
#install.packages("ggpubr")
#install.packages("ggbeeswarm")
#install.packages("cutpointr")
library(dplyr)
library(ggplot2)
library(MASS)
library(scales)
library(gridExtra)
library(cowplot)
library(ggpmisc)
library(pROC)
library(ggpubr)
library(ggbeeswarm)
library(cutpointr)
```

Function to determine coverage, average read depth (i.e. gene copies), and detection parameters of each target
Adjust the minimum coverage (min_coverage) and minimum read distribution (min_distribution) parameters to fit your needs
```{r}
mapping_analysis <- function(sample_name, mapping_results, target_lengths, target_name) {
  # Minimum coverage and read distribution parameters
  min_coverage = 0.1
  min_distribution = 0.3
  
  # make a list of the standards ID in the mapping file
  targets = data.frame(unique(mapping_results$ID))
  colnames(targets) <- "ID"
  targets <- merge.data.frame(targets, target_lengths, by = "ID")
  colnames(targets) <- c("ID", "length")
  targets$cover_count <- 0
  targets$B_G <- 0
  targets$gene_copies <- 0
  targets$I_G <- 0
  targets$E_rel <- 0
  targets$C_o <- 0
  targets$C_e <- 0
  targets$R_FVE <- 0
  targets$LOD <- 0
  
  for(i in 1:nrow(targets)) {
    # select genome information on specific genome from mapping table
    target = subset(mapping_results, ID == targets$ID[i])
    target$I_G_x <- 0
    targets$cover_count[i] = nrow(subset(target, read_depth > 0))
    
    # Calculate total number of bases mapping to genome and the average read depth (gene copies)
    targets$B_G[i] = sum(target$read_depth)
    targets$gene_copies[i] = mean(target$read_depth)
    
    # Calculate I_G and number of positions covered by at least 1 read
    target$I_G_x <- (target$read_depth/targets$B_G[i])*log(target$read_depth/targets$B_G[i])
    targets$I_G[i] <- -sum(na.omit(target$I_G_x))
    
    # Calculate E_rel
    targets$E_rel[i] = targets$I_G[i]/log(targets$length[i])
    
    # FastViromeExplorer LOD parameters
    targets$C_o[i] = targets$cover_count[i]/targets$length[i]
    targets$C_e[i] = 1 - exp(-targets$B_G[i]/targets$length[i])
    if(!(is.na(targets$gene_copies[i]))) {
      if(targets$C_e[i] != 0) {
        targets$R_FVE[i] = targets$C_o[i]/targets$C_e[i]
      }
      if(targets$C_o[i] > min_coverage & targets$R_FVE[i] > min_distribution) {
        targets$LOD[i] = 1 # Passed LOD requirement
      }
      if(!(targets$C_o[i] > min_coverage & targets$R_FVE[i] > min_distribution)) {
        targets$LOD[i] = 0 # Failed LOD requirement
      }
    }
    
  }
  
  mapping_targets_logit = data.frame(cbind(targets$ID, targets$E_rel, targets$LOD))
  colnames(mapping_targets_logit) <- c("ID", "E_rel", "LOD")
  mapping_targets_analysis = data.frame(cbind(targets$ID, targets$E_rel, targets$C_o, targets$R_FVE, targets$B_G, targets$gene_copies))
  colnames(mapping_targets_analysis) <- c("ID", "E_rel", "C_o", "R_FVE", "B_G", "gene_copies")
  
  output = "Regressions/detection/sample_db_mapping_analysis.txt"
  output = gsub("sample", sample_name, output)
  output = gsub("db", target_name, output)
  write.table(mapping_targets_analysis, file = output, append = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
  output2 = "Regressions/detection/sample_db_logit.txt"
  output2 = gsub("sample", sample_name, output2)
  output2 = gsub("db", target_name, output2)
  write.table(mapping_targets_logit, file = output2, append = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
  
  return(mapping_targets_analysis)
} 
```

### Upload information on the spiked-in standards
Update the code with the specific mix of sequins spiked in (if using a different file from STD_MIXES.txt must include length of each standard in a separate column)
```{r}
STDS <- as.data.frame(read.table("Spike-ins/STD_MIXES.txt", sep = "\t", header = TRUE, stringsAsFactors = FALSE))
# Change MIX_A to MIX_B or MIX_C depending on spike-in mix used
STDS_list <- subset(STDS, !(is.na(MIX_A)))

# Update to "NO" if ssDNA standards were not spiked into the samples
ssDNA_spike = "YES"

if (ssDNA_spike == "YES") {
 STDS_list <- rbind.data.frame(STDS_list, subset(STDS, !(is.na(ssDNA)))) 
}

STDS_list <- data.frame(STDS_list$ID, STDS_list$length)
colnames(STDS_list) <- c("ID", "length")
```

### Create and upload logistic regression model data for each sample (including downsampling) from running "Snakefile_map_standards" and "Snakefile_downsample"
Create lists of samples with respective downsampling
```{r}
# Update sample list with all of the samples (including no downsampling and downsampling)
### sample_descript provides direction to the directory with each Mapping/{sample_descript}/standards_mapping.txt file
### sample provides the sample name of the sample for the respective Mapping/{sample_descript}/standards_mapping.txt file
### downsample provides the fraction of reads used for the mapping in the Mapping/{sample_descript}/standards_mapping.txt file

samples <- data.frame("sample_descript"=c("example_1", "example_2"), "sample"=c("example_1", "example_2"), "downsample"=c(rep(100, 2)))

sample_list <- unique(samples$sample)
```

Upload mapping results and perform mapping analysis
```{r}
for (i in 1:nrow(samples)) {
  filename = "Mapping/sample/standards_mapping.txt"
  filename = gsub("sample", samples$sample_descript[i], filename)
  mapping_out <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
  mapping_target_analysis <- mapping_analysis(samples$sample_descript[i], mapping_out, STDS_list, "standards")
  
  filename = "Regressions/detection/sample_standards_logit.txt"
  filename = gsub("sample", samples$sample_descript[i], filename)
  all <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
  all$downsample <- rep(samples$downsample[i])
  all$sample <- rep(samples$sample[i])
  filename = "Regressions/detection/sample_standards_mapping_analysis.txt"
  filename = gsub("sample", samples$sample_descript[i], filename)
  results <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
  results$downsample <- rep(samples$downsample[i])
  results$sample <- rep(samples$sample[i])
  all <- cbind.data.frame(all, results$gene_copies)
  colnames(all) <- c("ID", "E_rel", "LOD", "downsample", "sample", "gene_copies")
    
  if (i == 1) {
    logit <- all
    mapping_results <- results
  } else {
    logit <- rbind.data.frame(logit, all)
    mapping_results <- rbind.data.frame(mapping_results, results)
  }
}
```

### Names of the mutated standards from running "Snakefile_failure_standards"
List out all of the failure standards mapping for four rounds of mutations
```{r}
# Update sample list with all of the samples (all downsampling will be 100%)
### sample_descript provides direction to the directory with each Mapping/{sample}/fail_standards_r{1-4}_mapping.txt file
### sample provides the sample name of the sample for the respective Mapping/{sample}/fail_standards_r{1-4}_mapping.txt file
### downsample will be 100% for all failure standards

fail_samples <- data.frame("sample_descript"=c("example_1_fail_r1", "example_2_fail_r1", "example_1_fail_r2", "example_2_fail_r2", "example_1_fail_r3", "example_2_fail_r3", "example_1_fail_r4", "example_2_fail_r4"), "sample"=c(rep(c("example_1", "example_2"), 4)), "downsample"=c(rep(100, 8)), "fail_set"=c(rep(c("fail_standards_r1"), 2), rep(c("fail_standards_r2"), 2), rep(c("fail_standards_r3"), 2), rep(c("fail_standards_r4"), 2)))

fail_set = c("fail_standards_r1", "fail_standards_r2", "fail_standards_r3", "fail_standards_r4")
```

Lengths of failure standards
```{r}
for (i in fail_set) {
  filename = "Map_Indexes/fail_set/fail_standards_lengths.txt"
  filename = gsub("fail_set", i, filename)
  f_std <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
  colnames(f_std) <- c("ID", "length")
  f_std$fail_set <- i
  
  if (i == "fail_standards_r1") {
    F_STD = f_std
  } else {
    F_STD = rbind.data.frame(F_STD, f_std)
  }
}
```

Upload failure standards mapping results and perform mapping analysis
```{r}
for (i in 1:nrow(fail_samples)) {
  filename = "Mapping/sample/fail_set_mapping.txt"
  filename = gsub("sample", fail_samples$sample[i], filename)
  filename = gsub("fail_set", fail_samples$fail_set[i], filename)
  mapping_out <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
  f_std <- subset(F_STD, fail_set == fail_samples$fail_set[i])
  mapping_target_analysis <- mapping_analysis(fail_samples$sample[i], mapping_out, f_std, fail_samples$fail_set[i])
  
  filename = "Regressions/detection/sample_db_logit.txt"
  filename = gsub("sample", fail_samples$sample[i], filename)
  filename = gsub("db", fail_samples$fail_set[i], filename)
  all <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
  all$fail_set <- rep(fail_samples$fail_set[i])
  all$sample <- rep(fail_samples$sample[i])
  filename = "Regressions/detection/sample_db_mapping_analysis.txt"
  filename = gsub("sample", fail_samples$sample[i], filename)
  filename = gsub("db", fail_samples$fail_set[i], filename)
  results <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
  results$fail_set <- rep(fail_samples$fail_set[i])
  results$sample <- rep(fail_samples$sample[i])
  all <- cbind.data.frame(all, results$gene_copies)
  colnames(all) <- c("ID", "E_rel", "LOD", "fail_set", "sample", "gene_copies")
    
  if (i == 1) {
    fail_logit <- all
    fail_mapping_results <- results
  } else {
    fail_logit <- rbind.data.frame(fail_logit, all)
    fail_mapping_results <- rbind.data.frame(fail_mapping_results, results)
  }
}

fail_logit <- subset(fail_logit, LOD == 0)
fail_mapping_results <- subset(fail_mapping_results, (sample %in% fail_logit$sample) & (fail_set %in% fail_logit$fail_set) & (ID %in% fail_logit$ID))

# Combine the fail set of standards with the rest of the standards
logit$fail_set <- "Not_applicable"
fail_logit$downsample <- 100

mapping_results$fail_set <- "Not_applicable"
fail_mapping_results$downsample <- 100

logit <- rbind.data.frame(logit, fail_logit)
mapping_results <- rbind.data.frame(mapping_results, fail_mapping_results)
```

### Generate logistic regression model based on the results of the standard datasets
```{r}
standards.lr <- glm(LOD ~ E_rel, data = logit, family = binomial(link = "logit"))
```

### Import test dataset(s)
# Recommend mapping reads to databases with a wide variety of lengths (in Langenfeld et al. 2022, test datasets included mapping reads to NCBI DNA viral genes, NCBI DNA viral genomes, and the VirSorter curated viral databases)
Update the database list with names of databases reads were mapped to (there should be two files for each database name: Map_Indexes/{db_name}_lengths.txt and Mapping/{sample_name}/{db_name}_mapping.txt)
```{r}
databases <- c("NCBI_viral_genomes")#, "VirSorter_curated_db")
x = 1

for (j in databases) {
  filename = "Map_Indexes/db_lengths.txt"
  filename = gsub("db", j, filename)
  lengths <- as.data.frame(read.table(filename, sep = "\t", header = FALSE, stringsAsFactors = FALSE))
  colnames(lengths) <- c("ID", "length")
  
  df_title = "db_length"
  df_title = gsub("db", j, df_title)
  assign(df_title, lengths)
  
  for (i in 1:nrow(samples)) {
    filename = "Mapping/sample/db_mapping.txt"
    filename = gsub("sample", samples$sample_descript[i], filename)
    filename = gsub("db", j, filename)
    mapping_out <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
    mapping_target_analysis <- mapping_analysis(samples$sample[i], mapping_out, lengths, j)
  
    filename = "Regressions/detection/sample/db_logit.txt"
    filename = gsub("sample", samples$sample[i], filename)
    filename = gsub("db", j, filename)
    all <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
    all$downsample <- rep(samples$downsample[i])
    all$sample <- rep(samples$sample[i])
    filename = "Regressions/detection/sample/db_mapping_analysis.txt"
    filename = gsub("sample", samples$sample[i], filename)
    filename = gsub("db", j, filename)
    results <- as.data.frame(read.table(filename, sep = "\t", header = TRUE, stringsAsFactors = FALSE))
    results$downsample <- rep(samples$downsample[i])
    results$sample <- rep(samples$sample[i])
    all <- cbind.data.frame(all, results$gene_copies)
    colnames(all) <- c("ID", "E_rel", "LOD", "downsample", "sample", "gene_copies")
    
    if (i == 1) {
      db_logit <- all
      db_mapping_results <- results
    } else {
      db_logit <- rbind.data.frame(db_logit, all)
      db_mapping_results <- rbind.data.frame(db_mapping_results, results)
    }
  }
  
  df_title <- "db_logit"
  df_title <- gsub("db", j, df_title)
  assign(df_title, db_logit)
  
  df_title <- "db_mapping_results"
  df_title <- gsub("db", j, df_title)
  assign(df_title, db_mapping_results)
  
  if (x == 1) {
    all_db_logit <- db_logit
    all_db_mapping_results <- db_mapping_results
    all_db_length <- lengths
  } else {
    all_db_logit <- rbind.data.frame(all_db_logit, db_logit)
    all_db_mapping_results <- rbind.data.frame(all_db_mapping_results, db_mapping_results)
    all_db_length <- rbind.data.frame(all_db_length, lengths)
  }
  
  x = x + 1
}

```

# Functions to assess cutpoints of targets from databases
```{r}
logreg_assess <- function(test_data, sample_name) {
  assessment <- data.frame(sample_name)
  cp <- cutpointr(test_data, E_rel, above_LOD, method = maximize_boot_metric, metric = sum_sens_spec)
  assessment$ROC_AUC = cp$AUC
  assessment$optimal_cutpoint = cp$optimal_cutpoint
  colnames(assessment) <- c("sample", "ROC_AUC", "optimal_cutpoint")
  return(assessment)
}
```

### Bin the database mapping results by the target length, then determine the optimal E_rel cutpoint for each length bin to set as the optimal detection threshold
The length bins are currently relevant for individual genes and virus genomes.
The bins should be updated if longer sequences, such as prokaryote genomes, are being assessed in metagenomes.
```{r}
all_db_logit <- merge(all_db_logit, all_db_length, by = "ID")
all_db_logit <- arrange(all_db_logit, length)

# bin by length, update the bins based on the lengths of the databases (check target length distribution using the following)
### min_length <- min(all_db_logit$length)
### max_length <- max(all_db_logit$length)
### median_length <- median(all_db_logit$length)

length_bins <- data.frame(c(500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 12500, 15000, 17500, 20000, 22500, 25000, 27500, 30000, 35000, 40000, 50000, 60000, 75000, 100000, 200000))

# Determine the optimal cutpoint for each length bin
for (i in 1:(nrow(length_bins)+1)) {
  if (i == 1) {
    bin <- subset(all_db_logit, length <= length_bins[i,1])
    name <- "<=len_bin"
    name <- gsub("len_bin", length_bins[i,1], name)
    results_lengths <- logreg_assess(bin, name)
  } else if (i == (nrow(length_bins)+1)) {
    bin <- subset(all_db_logit, length > length_bins[i-1,1])
    name <- ">len_bin"
    name <- gsub("len_bin", length_bins[i-1,1], name)
    results_lengths <- rbind.data.frame(results_lengths, logreg_assess(bin, name))
  }
  else {
    bin <- subset(all_0_v2, length <= length_bins[i,1] & length > length_bins[i-1,1])
    name <- "sbin-lbin"
    name <- gsub("sbin", length_bins[i-1,1], name)
    name <- gsub("lbin", length_bins[i,1], name)
    results_lengths <- rbind.data.frame(results_lengths, logreg_assess(bin, name))
  }
}

colnames(results_lengths) <- c("sample", "AUC", "E_rel")
results_lengths$fit <- 0
results_lengths$se.fit <- 0
results_lengths$residual.scale <- 0
results_lengths$UL <- 0
results_lengths$LL <- 0
results_lengths$PredictedProb <- 0

for (i in 1:nrow(results_lengths)) {
  results_lengths[i,4:6] <- predict(standards.lr, newdata = results_lengths[i,], type = "link", se = TRUE)
  results_lengths[i,] <- within(results_lengths[i,], {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
  })
}

# Update the median length to fit with any changes made to "length_bins" above
results_lengths$median_length <- c(250, 750, 1500, 2500, 3500, 4500, 5500, 6500, 7500, 8500, 9500, 11250, 13750, 16250, 18750, 21250, 23750, 26250, 28750, 32500, 37500, 45000, 55000, 67500, 87500, 150000, median(subset(all_db_logit, length > 200000)$length))

colnames(results_lengths) <- c("length bin", "AUC", "optimal cutpoint", "fit", "se.fit", "residual.scale", "UL", "LL", "PredictedProb", "median length")

# alter my.formula to improve the fit (e.g., increase R2)
my.formula <- y ~ log10(log10(log10(x)))

# plot the detection threshold results 
results_lengths %>% 
  ggplot(aes(x=`median length`, y=`optimal cutpoint`, color=PredictedProb)) + 
  geom_smooth(method = "lm", se = FALSE, color = "black", formula = my.formula) +
  stat_poly_eq(formula = my.formula, aes(label = paste(..eq.label.., ..rr.label.., sep = "~~"), size = 12), parse = TRUE) +
  geom_point() + 
  pretty_plot + 
  scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x), labels = trans_format("log10", math_format(10^.x))) +
  xlab("Reference Target Length (bp)") + ylab(expression(paste("Optimal ", R[G], " Cutpoint"))) + 
  scale_color_continuous(name="Logistic \nRegression \nProbability")

# Save the plot to the Regressions/detection directory
ggsave("Regressions/detection/detection_threshold.png", plot = last_plot(), scale = 1, width = 7, height = 5, dpi = 400, units = "in", limitsize = TRUE)
```

# Save the detection thresholds with respect to target length
Update "my.formula" so it matches the above optimal format so x and y are updated to length and target_cutpoint, respectively
```{r}
colnames(results_lengths) <- c("length bin", "AUC", "target_cutpoint", "fit", "se.fit", "residual.scale", "UL", "LL", "PredictedProb", "length")

# Update forula to match the section above 
my.formula <- target_cutpoint ~ log10(log10(log10(length)))

# save the linear model results to min_E_rel
E_detect <- lm(my.formula, results_lengths)

# save the linear model to the Regressions/detection directory
saveRDS(E_detect, file = "Regressions/detection/detection_threshold")
```

### Update the Snakefile_quant_unknown and Snakefile_quantmeta files to include this E_detect threshold
